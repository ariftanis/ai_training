# Model Configuration
MODEL_NAME=unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit
MAX_SEQ_LENGTH=8192

# LoRA Configuration
LORA_RANK=128
LORA_ALPHA=32
LORA_DROPOUT=0.05

# Training Configuration
NUM_EPOCHS=5
BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
WARMUP_STEPS=50
LEARNING_RATE=2e-4

# Dataset Processing
DATASET_NUM_PROC=2

# Model Output Configuration
OUTPUT_DIR=outputs
SAVE_STRATEGY=epoch

# GGUF Export Configuration
GGUF_OUTPUT_NAME=sancaktepe-model.gguf