# Model Configuration
MODEL_NAME=unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit
MAX_SEQ_LENGTH=8192

# LoRA Configuration
LORA_RANK=128
LORA_ALPHA=32
LORA_DROPOUT=0.05

# Training Configuration
NUM_EPOCHS=2
BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=8
WARMUP_STEPS=50
LEARNING_RATE=2e-4

# Dataset Processing
DATASET_NUM_PROC=2

# Model Output Configuration
OUTPUT_DIR=outputs
SAVE_STRATEGY=epoch

# GGUF Export Configuration
GGUF_OUTPUT_NAME=Meta-Llama-3.1-8B-Instruct-bnb-4bit-sancaktepe.gguf
GGUF_QUANTIZATION=q8_0

# Valid quantization options: q4_k_m, q5_k_m, q8_0, f16, f32
# q4_k_m: Good balance of size/speed/quality (recommended for most use cases)
# q5_k_m: Higher quality than q4_k_m, larger file size
# q8_0: Near original quality, larger file size than q4/q5
# f16: Full precision, largest file size

#"unsloth/Meta-Llama-3.1-8B-bnb-4bit",      # Llama-3.1 15 trillion tokens model 2x faster!
#"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit",
#"unsloth/Meta-Llama-3.1-70B-bnb-4bit",
#"unsloth/Meta-Llama-3.1-405B-bnb-4bit",    # We also uploaded 4bit for 405b!
#"unsloth/Mistral-Nemo-Base-2407-bnb-4bit", # New Mistral 12b 2x faster!
#"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit",
#"unsloth/mistral-7b-v0.3-bnb-4bit",        # Mistral v3 2x faster!
#"unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
#"unsloth/Phi-3.5-mini-instruct",           # Phi-3.5 2x faster!
#"unsloth/Phi-3-medium-4k-instruct",
#"unsloth/gemma-2-9b-bnb-4bit",
#"unsloth/gemma-2-27b-bnb-4bit",            # Gemma 2x faster!
# hello