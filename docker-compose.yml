version: '3.8'

services:
  qlora-llm:
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      # Mount the documents directory to provide data for preparation
      - ./documents:/app/documents:ro
      # Mount the dataset file so it's persisted after creation
      - ./dataset.jsonl:/app/dataset.jsonl
      # Mount the output model directory to persist the trained model on your host machine
      - ./my-finetuned-model:/app/my-finetuned-model
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # When running `docker-compose up`, the default CMD from the Dockerfile will be used.
    # To run inference with a custom prompt, use the `run` command:
    # docker-compose run --rm qlora-llm python3 src/inference.py "Your custom prompt here"