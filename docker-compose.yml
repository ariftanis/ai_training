version: '3.8'

services:
  qlora-llm:
    build:
      context: .
      dockerfile: Dockerfile
    network_mode: "bridge"  # Use bridge mode but add host alias
    extra_hosts:
      - "host.docker.internal:host-gateway"  # This maps to the host gateway IP in Docker
    volumes:
      # Mount the dataset file so it's persisted after creation
      - ./dataset.jsonl:/app/dataset.jsonl
      # Mount the output model directory to persist the trained model on your host machine
      - ./my-finetuned-model:/app/my-finetuned-model
      # Mount GGUF output directory to persist GGUF models on your host machine
      - ./output:/app/output
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # When running `docker-compose up`, the default CMD from the Dockerfile will be used.
    # To run inference with a custom prompt, use the `run` command:
    # docker-compose run --rm qlora-llm python3 src/inference.py "Your custom prompt here"